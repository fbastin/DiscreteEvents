\documentclass[t,usepdftitle=false]{beamer}
\usepackage[utf8]{inputenc}
\usetheme{Warsaw}
\usepackage{xcolor}

\usepackage{etex}
\usepackage{pictex}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\usepackage{pgfplots}

\usepackage{amsmath}
\usepackage{amscd}
\usepackage{pstricks, pst-node}

\usepackage{times}

\usepackage{ulem}

\usepackage{amsmath, amsthm}
\usepackage{listings}

% \setbeamercovered{transparent}
%\usecolortheme{crane}
\title[IFT3245]{IFT 3245\\Simulation et modèles}
\author[Fabian Bastin]{Fabian Bastin\\DIRO\\Université de Montréal}
\date{Automne 2016}

\def\be{\boldsymbol{e}}
\def\bh{\boldsymbol{h}}
\def\bu{\boldsymbol{u}}
\def\bv{\boldsymbol{v}}
\def\bx{\boldsymbol{x}}
\def\bA{\boldsymbol{A}}
\def\bP{\boldsymbol{P}}
\def\bX{\boldsymbol{X}}
\def\bY{\boldsymbol{Y}}
\def\bmu{\boldsymbol{\mu}}
\def\bSigma{\boldsymbol{\Sigma}}
\def\bzero{\boldsymbol{0}}

\def\cH{\mathcal{H}}
\def\cJ{\mathcal{J}}
\def\cS{\mathcal{S}}

\def\rit{\mathcal{R}}
\def\NN{\mathcal{N}}
\def\RR{\mathcal{R}}
\def\ZZ{\mathcal{Z}}

\def\eps {$< 10^{-15}$}
\def\epsm {$> 1-10^{-15}$}

\def\MSE{\mbox{MSE}}
\def\RE{\mbox{RE}}
\def\eff{\mbox{Eff}}
\def\Var{\mbox{Var}}
\def\Cov{\mbox{Cov}}
\def\var{\mbox{var}}

\def\iid{i.i.d.}
\def\toas{\mbox{ to as }}
\def\To{\overset{D}{\to}}

\newtheorem{thm}{Theorème}
\newtheorem{coro}{Corollaire}
\newtheorem{prop}{Proposition}
\newtheorem{exe}{Exemple}

\begin{document}
\frame{\titlepage}

\begin{frame}
\frametitle{Intervalle de confiance pour une fonction de plusieurs
  moyennes}
\label{sec:foncmoy}

Dans le cas déterministe, nous savons que si ${\bY_n} =
(Y_{1n},\dots,Y_{dn})$ converge vers un certain vecteur $\bmu =
(\mu_1,\dots,\mu_d)$ et si ${g} : \RR^d\to \RR$ est continue, alors
$g(\bY_n)\to g(\bmu)$.

\mbox{}

Supposons à présent que les $\bY_n$ sont des vecteurs
  aléatoires et que ${r(n)} (\bY_n-\bmu) \To \bY$.

\mbox{}

Par exemple, si $\bY_n$ est une moyenne de $n$ vecteurs, nous
savons que  $\sqrt{n}(\bY_n-\bmu) \To N(\bzero,\bSigma_y)$.

\mbox{}

Avons-nous encore la convergence de
$r(n)(g(\bY_n) - g(\bmu))$, et le cas échéant, vers quelle distribution?

\end{frame}

\begin{frame}
\frametitle{Théorème Delta}

\begin{thm}[Théorème Delta]
Soit ${g} :\RR^d\to\RR$ continûment différentiable 
dans un voisinage de $\bmu$, et $\nabla g$ son gradient.
Si ${r(n)}(\bY_n-\bmu) \To \bY$ quand $n\to\infty$, alors
\[
  r(n) (g(\bY_n)-g(\bmu)) \To (\nabla g(\bmu))^T \bY 
                                \quad\mbox{ quand } n\to\infty.
\]
\end{thm}

\mbox{}

\begin{coro}[Corollaire]
Si $\sqrt{n}(\bY_n - \bmu) \To N(\bzero,\bSigma_y)$ quand $n\to\infty$,
alors on a le TLC:
\[
  \sqrt{n}(g(\bY_n) - g(\bmu))/\sigma_g \To N(0,1)
  \quad\mbox{ quand } n\to\infty,
\] 
où ${\sigma_g}^2 = (\nabla g(\bmu))^T\bSigma_y\nabla g(\bmu)$.
\end{coro}

\end{frame}

\begin{frame}
\frametitle{Quotient de deux espérances}

Soient $(X_1,Y_1),\dots,(X_n,Y_n)$ des copies \iid{} de ${(X,Y)}$
et supposons que l'on estime ${\nu} = E[X]/E[Y]$ par
$$
  {\hat\nu_n} = {\overline{X}_n \over \overline{Y}_n}
                  = {\sum_{i=1}^n X_i \over \sum_{i=1}^n Y_i}.
$$

\mbox{}

Cet estimateur est biaisé mais fortement consistant.

\mbox{}

Posons ${\mu_1} = E[X]$, ${\mu_2} = E[Y]$, 
${g}(\mu_1,\mu_2) = \mu_1/\mu_2$,
${\sigma_1^2} = \Var[X]$,
${\sigma_2^2} = \Var[Y]$, et
${\sigma_{12}} = \Cov[X, Y]$.
Supposons que ces quantités soient finies et que
$\mu_2\not=0$, $\sigma_1^2 > 0$, et $\sigma_2^2 > 0$.

\mbox{}

Le gradient de $g$ est
\[
{\nabla g(\mu_1,\mu_2)} = (1/\mu_2,\, -\mu_1/\mu_2^2)^T.
\]

\end{frame}

\begin{frame}
\frametitle{Quotient de deux espérances}

En vertu du théorème de la limite centrale,
\[
  \sqrt{n} (\overline{X}_n-\mu_1, \overline{Y}_n-\mu_2)^T
    \To ({W_1}, {W_2})^T 
   \sim N(\bzero,\bSigma)
\]
où
\[
  \bSigma =
  \begin{pmatrix}
    \sigma_1^2 & \sigma_{12} \\
    \sigma_{12} & \sigma_2^2
  \end{pmatrix}.
\]
Puis, par le théorème delta (ou son corollaire), nous avons
\[
  \sqrt{n}(\hat\nu_n-\nu) \To (W_1, W_2)\cdot \nabla g(\mu_1,\mu_2) 
   = W_1/\mu_2 - W_2 \mu_1/\mu_2^2
   \sim N(0,\sigma_g^2)
\]
où
\begin{align*}
 {\sigma_g^2}
            & = (\nabla g(\bmu))^T\bSigma_y\nabla g(\bmu) \\
            & = \sigma_1^2/\mu_2^2 + \sigma_{2}^2 \mu_1^2/\mu_2^4 
                - 2 \sigma_{12} \mu_1/\mu_2^3,
\end{align*}
ou encore
\[
%\begin{equation}
 {\sigma_g^2} = \frac{\sigma_1^2 + \sigma_{2}^2 \nu^2 - 2 \sigma_{12} \nu}{\mu_2^2}.
%\label{eq:sigma_g}
%\end{equation}
\]

\end{frame}

\begin{frame}
\frametitle{Quotient de deux espérances}

Nous pouvons calculer un intervalle de confiance en utilisant ce
dernier théorème de la limite centrale si nous disposons d'un bon
estimateur de $\sigma^2_g$.
Un candidat évident est:
\[
 {\hat\sigma_{g,n}^2} =
 \frac{\hat\sigma_1^2 + \hat\sigma_{2}^2 \hat\nu_n^2 - 2
   \hat\sigma_{12} \hat\nu_n}{\overline{Y}_n^2},
\]
où
\begin{eqnarray*}
 \hat\sigma_1^2 &=& {1\over n-1} \sum_{j=1}^n (X_j - \overline{X}_n)^2,\\
 \hat\sigma_2^2 &=& {1\over n-1} \sum_{j=1}^n (Y_j - \overline{Y}_n)^2,\\
 \hat\sigma_{12} &=& {1\over n-1}
                    \sum_{j=1}^n (X_j - \overline{X}_n)(Y_j-\overline{Y}_n).
\end{eqnarray*}

\end{frame}

\begin{frame}
\frametitle{Quotient de deux espérances}

Puisque $\hat\sigma_{g,n}^2$ est fortement consistant, on obtient le théorème de la limite centrale
\[
     \frac{\sqrt{n}(\hat\nu_n-\nu)}{\hat\sigma_{g,n}} 
 \To \frac{\sqrt{n}(\hat\nu_n-\nu)}{\sigma_{g}} 
 \To N(0,1) \qquad\mbox { quand } n\to\infty.
\]

\mbox{}

L'intervalle de confiance \emph{classique} pour $\nu$ au niveau nominal $1-\alpha$
est $(\hat\nu_n - r,\, \hat\nu_n + r)$ où 
${r} = z_{1-\alpha/2} \hat\sigma_{g,n} / \sqrt{n}$.

\mbox{}

Son erreur de couverture est parfois grande lorsque $n$ n'est pas
très grand, ou lorsque la convergence vers $N(0,1)$ est lente.

\mbox{}

Dans ce cas, on recommande d'utiliser le 
\emph{bootstrap-$t$ non-paramétrique}, en prenant
$\hat\nu_n$ et $\hat\sigma_{g,n}$ comme estimateurs de la 
moyenne et de la variance.

\end{frame}

\begin{frame}
\frametitle{Quotient de deux espérances}

Pour le cas particulier d'un rapport de deux espérances, 
la dérivation suivante est plus directe.

\mbox{}

Les variables aléatoires
$$
  {Z_j} = X_j - \nu Y_j,
$$
sont \iid\ de moyenne 0 et de variance
\begin{align*}
  {\sigma_z^2} = \Var[Z_j] & =
   \Var[X_j] + \nu^2\Var[Y_j] - 2\nu \Cov(X_j, Y_j)\\
& = \sigma_1^2 + \sigma_2^2 \nu^2 - 2\sigma_{12}\nu.
\end{align*}

\mbox{}

En appliquant le TLC aux $Z_j$, on obtient
\[
 {\sqrt{n}\overline{Y}_n (\hat\nu_n - \nu) \over \sigma_z}
 = {\sqrt{n} \overline{Z}_n \over \sigma_z}
   \To N(0,1) \qquad\mbox { quand } n\to\infty.
\]

\end{frame}

\begin{frame}
\frametitle{Quotient de deux espérances}

C'est \emph{équivalent}, car $\sigma_z / \overline{Y}_n$ converge vers $\sigma_z / \mu_2 = \sigma_g$ quand $n\to\infty$, presque sûrement.

\mbox{}

\emph{Remarque importante}: on préfère $\Cov(X_j, Y_j) > 0$!

\end{frame}

\begin{frame}
\frametitle{Différence entre deux moyennes}

On a ${n_1}$ observations \iid{} $X_{11},\dots, X_{1,n_1}$,
de moyenne ${\mu_1}$,
et ${n_2}$ observations \iid{} $X_{21},\dots, X_{2,n_2}$, 
de moyenne ${\mu_2}$.
On veut un intervalle de confiance pour ${\mu_1 - \mu_2}$.

\mbox{}

Les deux méthodes suivantes supposent que les $X_{ji}$
suivent la \emph{loi normale}. \\ (Pas toujours valide!)

\mbox{}

Dans la seconde (Welch), les deux échantillons doivent être 
indépendants mais on peut avoir $n_1\not= n_2$.
Dans la première, il faut $n_1 = n_2$ mais $X_{1i}$ et $X_{2i}$
peuvent être corrélés.

\end{frame}

\begin{frame}
\frametitle{Première approche: observations couplées}

Soit $n_1 = n_2 = {n}$.

\mbox{}

Posons ${Z_i} = X_{1i} - X_{2i}$ pour $1\le i\le n$,
$$
 \overline{Z}_n = {1\over n} \sum_{i=1}^n Z_i, \qquad\mbox{ et }\qquad
    S_n^2 = {1\over n-1} \sum_{i=1}^n (Z_i - \overline{Z})^2.
$$

\mbox{}

Puisque les $Z_i$ sont des normales \iid, 
\[
  \sqrt{n} [\overline{Z}_n - (\mu_1-\mu_2)]/S_n \sim \mbox{ \emph{Student}}(n-1).
\]

\mbox{}

On utilise cela pour calculer l'intervalle de confiance.
Puisque
\[
  \Var[Z_i] = \Var[X_{1i}] + \Var[X_{2i}] - 2 \Cov[X_{1i}, X_{2i}],
\]
il est avantageux d'avoir \emph{$\Cov[X_{1i}, X_{2i}] > 0$}.

\end{frame}

\begin{frame}
\frametitle{Deuxième approche: méthode de Welch}

On suppose que $X_{1i}$ et $X_{2i}$ sont indépendants.  Soit
$$
 {\overline{X}_{(k)}} = {1\over n_k} \sum_{i=1}^{n_k} X_{ki}
   \qquad\mbox{ et }\qquad
 {S_{(k)}^2} = {1\over n_k-1} \sum_{i=1}^{n_k} (X_{ki} -\overline{X}_{(k)})^2,
$$
pour $k=1,2$.

\mbox{}

Alors,
$$ {\overline{X}_{(1)} - \overline{X}_{(2)} - (\mu_1-\mu_2) \over
     [S_{(1)}^2/n_1 + S_{(2)}^2/n_2]^{1/2}}
     \approx \mbox{ \emph{Student}}(\hat\ell)
$$
où
$$
  {\hat\ell} = { [S_{(1)}^2/n_1 + S_{(2)}^2/n_2]^2 \over
                   [S_{(1)}^2/n_1]^2/(n_1-1) + [S_{(2)}^2/n_2]^2/(n_2-1)}.
$$
\end{frame}

\end{document}